# -*- coding: utf-8 -*-
"""TripletMarginLossMNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T2r6mcAaquaLQq78xHupI8cz92GWEA63
"""
from rich.console import Console

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from pytorch_lightning import LightningModule, Trainer

### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###
from torchvision import datasets, transforms

from pytorch_metric_learning import distances, losses, miners, reducers, testers
from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning import Callback
from pytorch_lightning.loggers import CSVLogger
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.callbacks import QuantizationAwareTraining

import wandb







class recognition_model(LightningModule):
    def __init__(self, conv1_N = 64, conv1_filter = 3 , conv1_stride = 1,
                 conv2_N = 128, conv2_filter = 5 , conv2_stride = 1 , dropout = 0.50, linear = [512, 256, 1024], lr = 0.001,
                 batch_size = 1024):
        super(recognition_model, self).__init__()
        # self.train_set = trainset
        # self.test_set = testset
        self.lr = lr
        self.batch_size = batch_size
        self.transforms_ = transforms.Compose(
                                            [transforms.ToTensor(), 
                                            transforms.Normalize((0.1307,), (0.3081,))]
                                            )           
        self.save_hyperparameters()

        self.conv1 = nn.Conv2d(1, conv1_N, conv1_filter, conv1_stride) # [N, 30, 30, 32]
        self.relu1 = nn.ReLU()
        self.relu2 = nn.ReLU()
        self.gelu1 = nn.GELU()
        self.gelu2 = nn.GELU()
        self.conv2 = nn.Conv2d(conv1_N, conv2_N, conv2_filter, conv2_stride) # [N, 28, 28, 64]
        self.dropout1 = nn.Dropout2d(dropout)
        self.dropout2 = nn.Dropout2d(dropout)
        self.fc1 = nn.Linear(56448, linear[0])
        self.fc2 = nn.Linear(linear[0], linear[1])
        self.fc3 = nn.Linear(linear[1], linear[2])


        # metrics
        self.distance = distances.CosineSimilarity()
        self.reducer = reducers.ThresholdReducer(low=0)
        self.loss_func = losses.TripletMarginLoss(margin=0.2, distance=self.distance, reducer=self.reducer)
        # self.loss_func = losses.CircleLoss(m=0.4, gamma=80)
        self.mining_func = miners.TripletMarginMiner(
            margin=0.2, distance=self.distance, type_of_triplets="semihard"
        )
        self.accuracy_calculator = AccuracyCalculator(include=("precision_at_1",), k=1)

    def forward(self, x):
        x = self.conv1(x)# [N, 30, 30, 32]
        x = self.relu1(x)# [N, 30, 30, 32]
        x = self.conv2(x)# [N, 28, 28, 64]
        x = self.relu2(x)# [N, 28, 28, 64]
        x = F.avg_pool2d(x, 2, 1)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        # Console().rule(title=f"after flatten ===> {x.shape}", style="red on black")
        x = self.fc1(x)
        x = self.gelu1(x)

        x = self.fc2(x)
        x = self.gelu2(x)

        x = self.fc3(x)
        return x

    def get_all_embeddings(self, dataset, model):
        tester = testers.BaseTester()
        return tester.get_all_embeddings(dataset, model)

    def training_step(self, batch, batch_idx):

        data, labels = batch
        embeddings = self.forward(data)
        # Console().rule(title=f"EMBEDDINGS shape ===> {embeddings.shape}", style="bold magenta")
        indices_tuple = self.mining_func(embeddings, labels)
        # Console().rule(title=f"INDICES shape ===> {indices_tuple}", style="bold magenta")
        loss = self.loss_func(embeddings, labels, indices_tuple)
        # Console().print(f"epoch --> {self.current_epoch}\t\t Loss --> {loss.tolist()}", style="bold cyan")
        self.log(
            "train_loss" , loss.tolist(), sync_dist=True
            )

        return {"loss" : loss}

    # def validation_step(self, batch, batch_idx):
    def training_epoch_end(self, training_step_outputs):
        # mod_quant = self.quant
        mod_quant = self
        train_embeddings, train_labels = self.get_all_embeddings(self.train_set, mod_quant)
        test_embeddings, test_labels = self.get_all_embeddings(self.test_set, mod_quant)
        train_labels = train_labels.squeeze(1)
        test_labels = test_labels.squeeze(1)
        # print("Computing accuracy")
        accuracies = self.accuracy_calculator.get_accuracy(
            test_embeddings, train_embeddings, test_labels, train_labels, False
        )
        # print(accuracies)
        self.log("val_acc", accuracies["precision_at_1"], sync_dist=True)
        # print("Test set accuracy (Precision@1) = {}".format(accuracies["precision_at_1"]))



    def configure_optimizers(self):
        return optim.AdamW(self.parameters(), lr=self.hparams.lr, amsgrad=False)
        # return {"lr_scheduler" : , "optimizer" : }

    def setup(self, stage=None):
        # states should be set (i.e. self.etc) here because it is called in each process
        self.train_set = datasets.MNIST(".", train=True, download=True, transform=self.transforms_)
        self.test_set = datasets.MNIST(".", train=False, transform=self.transforms_)


    def train_dataloader(self):
        train_loader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=8)
        return train_loader

    def val_dataloader(self):
        test_loader = torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size)
        return test_loader

    def test_dataloader(self):
        test_loader = torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size)
        return test_loader
    



batch_size = 512

wandb.login()



CONFIG = dict (
    dataset = "/content/MNIST",
    batch_size = batch_size,
    epochs = 10,
    # precision = 16,
    # amp_backend = "apex",
    # amp_level="O1",
    gpus = 2,
    dist_strategy="ddp",
    checkpoint_callback = ModelCheckpoint(monitor='val_acc',
                                      save_top_k=1,
                                      save_last=True,
                                      save_weights_only=False,
                                      filename='checkpoint/{epoch:03d}-{train_loss:.6f}-{val_acc:.6f}',
                                      verbose=True,
                                      mode='max'),

    qat = QuantizationAwareTraining(
                                    # specification of quant estimation quality
                                    observer_type="histogram",
                                    # specify which layers shall be merged together to increase efficiency
                                    modules_to_fuse=[("conv1", "relu1"), ("conv2", "relu2")],
                                    # make your model compatible with all original input/outputs, in such case the model is wrapped in a shell with entry/exit layers.
                                    input_compatible=True,
                                ),
    # plugins = "deepspeed",
    # auto_scale_batch_size = "power"

)





wandb_logger = WandbLogger(project='train-siamese',
                           config=CONFIG,
                           group='MNIST',
                           job_type='Triplet loss + QAT + 2 GPU' ,
                           log_model="all", )

model = recognition_model(batch_size=CONFIG["batch_size"])
wandb_logger.watch(model, log="all")


trainer = Trainer(max_epochs = CONFIG["epochs"],
                    # precision=CONFIG["precision"],
                    # amp_backend=CONFIG["amp_backend"],
                    # amp_level=CONFIG["amp_level"],
                    gpus=CONFIG["gpus"],
                    strategy=CONFIG["dist_strategy"],
                   callbacks=[CONFIG["checkpoint_callback"], CONFIG["qat"]],
                    # callbacks=[CONFIG["checkpoint_callback"]],
                    logger=wandb_logger,
                #   auto_scale_batch_size=CONFIG["auto_scale_batch_size"],
                    weights_summary='top',
                    # plugins=CONFIG['plugins']
                    )


# trainer.tune(model)
trainer.fit(model)
wandb.finish()

